{"nbformat_minor": 2, "cells": [{"execution_count": 2, "cell_type": "code", "source": "!pip install geopy\n!pip install pandas\n!pip install pathos\n!pip install xgboost\n!pip install azure-storage\n!pip install reverse_geocoder", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Collecting geopy\n  Downloading geopy-1.11.0-py2.py3-none-any.whl (66kB)\n\u001b[K    100% |################################| 71kB 2.4MB/s ta 0:00:01\n\u001b[?25hInstalling collected packages: geopy\nSuccessfully installed geopy-1.11.0\nRequirement already satisfied: pandas in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages\nRequirement already satisfied: python-dateutil in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied: pytz>=2011k in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied: numpy>=1.7.0 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied: six>=1.5 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from python-dateutil->pandas)\nCollecting pathos\n  Downloading pathos-0.2.1.zip (82kB)\n\u001b[K    100% |################################| 92kB 2.8MB/s ta 0:00:011\n\u001b[?25hCollecting ppft>=1.6.4.7 (from pathos)\n  Downloading ppft-1.6.4.7.1.zip (78kB)\n\u001b[K    100% |################################| 81kB 3.8MB/s ta 0:00:011\n\u001b[?25hCollecting dill>=0.2.7 (from pathos)\n  Downloading dill-0.2.7.1.tar.gz (64kB)\n\u001b[K    100% |################################| 71kB 4.7MB/s ta 0:00:011\n\u001b[?25hCollecting pox>=0.2.3 (from pathos)\n  Downloading pox-0.2.3.zip (41kB)\n\u001b[K    100% |################################| 51kB 6.0MB/s ta 0:00:011\n\u001b[?25hCollecting multiprocess>=0.70.5 (from pathos)\n  Downloading multiprocess-0.70.5.zip (1.5MB)\n\u001b[K    100% |################################| 1.5MB 806kB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pathos, ppft, dill, pox, multiprocess\n  Running setup.py bdist_wheel for pathos ... \u001b[?25l-\b \bdone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/6b/b1/41/c44a53e7e20d98578cf5c14b7aac72822c6416f6439a45d349\n  Running setup.py bdist_wheel for ppft ... \u001b[?25l-\b \bdone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/4d/28/cc/c5bca1e1d3923a5f4dd50219a9e41391bbeb832835e7905344\n  Running setup.py bdist_wheel for dill ... \u001b[?25l-\b \bdone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/e5/88/fe/7e290ce5bb39d531eb9bee5cf254ba1c3e3c7ba3339ce67bee\n  Running setup.py bdist_wheel for pox ... \u001b[?25l-\b \bdone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/ff/b6/98/d03a0a0f153267d1601c3cbaa888ec4cdd52d658a945d44f15\n  Running setup.py bdist_wheel for multiprocess ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/28/ef/9f/5cc70b5d92fc4641b68dc23b3583f2b6ec1d153cb71985aeaf\nSuccessfully built pathos ppft dill pox multiprocess\nInstalling collected packages: ppft, dill, pox, multiprocess, pathos\n  Found existing installation: dill 0.2.5\n\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (dill) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n    Uninstalling dill-0.2.5:\n      Successfully uninstalled dill-0.2.5\nSuccessfully installed dill-0.2.7.1 multiprocess-0.70.5 pathos-0.2.1 pox-0.2.3 ppft-1.6.4.7.1\nCollecting xgboost\n  Downloading xgboost-0.6a2.tar.gz (1.2MB)\n\u001b[K    100% |################################| 1.2MB 937kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from xgboost)\nRequirement already satisfied: scipy in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from xgboost)\nRequirement already satisfied: scikit-learn in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from xgboost)\nBuilding wheels for collected packages: xgboost\n  Running setup.py bdist_wheel for xgboost ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/5e/c1/d6/522af54e5cc001fad4dd855117f8bf61b11d56443e06672e26\nSuccessfully built xgboost\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-0.6a2\nCollecting azure-storage\n  Downloading azure_storage-0.35.1-py2.py3-none-any.whl (190kB)\n\u001b[K    100% |################################| 194kB 2.7MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from azure-storage)\nRequirement already satisfied: azure-common>=1.1.5 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from azure-storage)\nRequirement already satisfied: cryptography in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from azure-storage)\nRequirement already satisfied: python-dateutil in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from azure-storage)\nRequirement already satisfied: azure-nspkg>=2.0.0 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from azure-storage)\nRequirement already satisfied: idna>=2.0 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cryptography->azure-storage)\nRequirement already satisfied: pyasn1>=0.1.8 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cryptography->azure-storage)\nRequirement already satisfied: six>=1.4.1 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cryptography->azure-storage)\nRequirement already satisfied: setuptools>=11.3 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cryptography->azure-storage)\nRequirement already satisfied: enum34 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cryptography->azure-storage)\nRequirement already satisfied: ipaddress in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cryptography->azure-storage)\nRequirement already satisfied: cffi>=1.4.1 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cryptography->azure-storage)\nRequirement already satisfied: pycparser in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from cffi>=1.4.1->cryptography->azure-storage)\nInstalling collected packages: azure-storage\nSuccessfully installed azure-storage-0.35.1\nCollecting reverse_geocoder\n  Downloading reverse_geocoder-1.5.1.tar.gz (2.2MB)\n\u001b[K    100% |################################| 2.3MB 513kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from reverse_geocoder)\nRequirement already satisfied: scipy>=0.17.1 in /home/nbcommon/anaconda2_410/lib/python2.7/site-packages (from reverse_geocoder)\nBuilding wheels for collected packages: reverse-geocoder\n  Running setup.py bdist_wheel for reverse-geocoder ... \u001b[?25l-\b \bdone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/92/7b/89/029b346ceab0416cd337ad2994a035b287bf54a0f22a4871c8\nSuccessfully built reverse-geocoder\nInstalling collected packages: reverse-geocoder\nSuccessfully installed reverse-geocoder-1.5.1\n"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "import pandas as pd\nfrom azure.storage.file import FileService\nimport os\n\naccount_name = 'kagglenyctaxi'\naccount_key = '9dX6B7LpElaR7rBBicZUmdUM5f5LkoT6TrWZd4rMRno9IP5BwVN0HojpSt90kHGTsMFMUaSEIxjs8LBbd7zw6g=='\nfile_service = FileService(account_name=account_name, account_key=account_key)\ndef get_train_data():\n    file_service.get_file_to_path('data', None, 'train.csv', 'train.csv')\ndef get_test_data():\n    file_service.get_file_to_path('data', None, 'test.csv', 'test.csv')\n\nif not os.path.exists('train.csv'):\n    get_train_data()\nif not os.path.exists('test.csv'):\n    get_test_data()\ndataframe = pd.read_csv('train.csv', index_col=0)\ntest_dataframe = pd.read_csv('test.csv', index_col=0)\nprint dataframe.head()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "           vendor_id      pickup_datetime     dropoff_datetime  \\\nid                                                               \nid2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \nid2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \nid3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \nid3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \nid2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n\n           passenger_count  pickup_longitude  pickup_latitude  \\\nid                                                              \nid2875421                1        -73.982155        40.767937   \nid2377394                1        -73.980415        40.738564   \nid3858529                1        -73.979027        40.763939   \nid3504673                1        -74.010040        40.719971   \nid2181028                1        -73.973053        40.793209   \n\n           dropoff_longitude  dropoff_latitude store_and_fwd_flag  \\\nid                                                                  \nid2875421         -73.964630         40.765602                  N   \nid2377394         -73.999481         40.731152                  N   \nid3858529         -74.005333         40.710087                  N   \nid3504673         -74.012268         40.706718                  N   \nid2181028         -73.972923         40.782520                  N   \n\n           trip_duration  \nid                        \nid2875421            455  \nid2377394            663  \nid3858529           2124  \nid3504673            429  \nid2181028            435  \n"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "from geopy.distance import great_circle\nfrom dateutil.parser import parse\nimport time\nfrom pathos.multiprocessing import ProcessingPool as Pool\nfrom pathos.multiprocessing import cpu_count\nimport numpy as np\n\n# def get_borough(lat, lng, retries=3):\n#     if retries <= -1: return None\n#     try:\n#         geolocator = Nominatim()\n#         location = geolocator.reverse('{}, {}'.format(lat, lng))\n#         return location.address.split(', ')[2]\n#     except Exception as e:\n#         print 'Too many requests. Waiting for 2 mins for {}, {}'.format(lat, lng)\n#         time.sleep(30)\n#         return get_borough(lat, lng, retries-1)\n        \ndef transform_df(df, great_circle=great_circle, parse=parse, np=np):\n    yes_func = lambda x: 1 if x == 'Y' else 0\n    df['distance'] = df.apply(lambda row : great_circle((row['pickup_latitude'], row['pickup_longitude']), \n                                                        (row['dropoff_latitude'], row['dropoff_longitude'])).miles, axis=1)\n                              \n    df['month'] = df.apply(lambda row: parse(row['pickup_datetime']).month, axis=1)\n    df['day'] = df.apply(lambda row: parse(row['pickup_datetime']).weekday(), axis=1)\n    df['pickup_hour'] = df.apply(lambda row: parse(row['pickup_datetime']).hour, axis=1)\n    df['store_and_fwd_flag'] = df.apply(lambda row: yes_func(row['store_and_fwd_flag']), axis=1)\n    return df\n\ndef parallelize_dataframe(df, func, num_cores, num_partitions):\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.clear()\n    return df\n\nnew_df = parallelize_dataframe(dataframe, transform_df, cpu_count(), cpu_count())\nnew_df['trip_duration'] = new_df.apply(lambda row: np.log1p(row['trip_duration']), axis=1)\nprint new_df.head()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "           vendor_id      pickup_datetime     dropoff_datetime  \\\nid                                                               \nid2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \nid2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \nid3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \nid3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \nid2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n\n           passenger_count  pickup_longitude  pickup_latitude  \\\nid                                                              \nid2875421                1        -73.982155        40.767937   \nid2377394                1        -73.980415        40.738564   \nid3858529                1        -73.979027        40.763939   \nid3504673                1        -74.010040        40.719971   \nid2181028                1        -73.973053        40.793209   \n\n           dropoff_longitude  dropoff_latitude  store_and_fwd_flag  \\\nid                                                                   \nid2875421         -73.964630         40.765602                   0   \nid2377394         -73.999481         40.731152                   0   \nid3858529         -74.005333         40.710087                   0   \nid3504673         -74.012268         40.706718                   0   \nid2181028         -73.972923         40.782520                   0   \n\n           trip_duration  distance  month  day  pickup_hour  \nid                                                           \nid2875421       6.122493  0.931400      3    0           17  \nid2377394       6.498282  1.122206      6    6            0  \nid3858529       7.661527  3.968634      1    1           11  \nid3504673       6.063785  0.923306      4    2           19  \nid2181028       6.077642  0.738763      3    5           13  \n"}], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "import reverse_geocoder as rg\nfrom sklearn import preprocessing\n\n# def get_borough_rev_geocoder(lat, lng):\n#     return rg.search((lat, lng))[0]['name']\n# dataframe['pickup_borough'] = dataframe.apply(lambda row: \n#                                               get_borough_rev_geocoder(row['pickup_latitude'], row['pickup_longitude']), axis=1)\ndef get_coords(df, lat_key, long_key):\n    lats = df[lat_key].values.tolist()\n    longs = df[long_key].values.tolist()\n    coords = zip(lats, longs)\n    return coords\ndef encode_labels(labels):\n    le = preprocessing.LabelEncoder()\n    le.fit(labels)\n    return le\ndef add_borough(df):\n    pickup_coords = get_coords(df, 'pickup_latitude', 'pickup_longitude')\n    dropoff_coords = get_coords(df, 'dropoff_latitude', 'dropoff_longitude')\n    pickup_boroughs = np.array([d['admin2'] for d in rg.search(pickup_coords)])\n    dropoff_boroughs = np.array([d['admin2'] for d in rg.search(dropoff_coords)])\n    df['pickup_borough'] = encode_labels(pickup_boroughs).transform(pickup_boroughs)\n    df['dropoff_borough'] = encode_labels(dropoff_boroughs).transform(dropoff_boroughs)\n    return df\nnew_df = add_borough(new_df)\nprint new_df.head()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "           vendor_id      pickup_datetime     dropoff_datetime  \\\nid                                                               \nid2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \nid2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \nid3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \nid3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \nid2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n\n           passenger_count  pickup_longitude  pickup_latitude  \\\nid                                                              \nid2875421                1        -73.982155        40.767937   \nid2377394                1        -73.980415        40.738564   \nid3858529                1        -73.979027        40.763939   \nid3504673                1        -74.010040        40.719971   \nid2181028                1        -73.973053        40.793209   \n\n           dropoff_longitude  dropoff_latitude  store_and_fwd_flag  \\\nid                                                                   \nid2875421         -73.964630         40.765602                   0   \nid2377394         -73.999481         40.731152                   0   \nid3858529         -74.005333         40.710087                   0   \nid3504673         -74.012268         40.706718                   0   \nid2181028         -73.972923         40.782520                   0   \n\n           trip_duration  distance  month  day  pickup_hour  pickup_borough  \\\nid                                                                            \nid2875421       6.122493  0.931400      3    0           17              26   \nid2377394       6.498282  1.122206      6    6            0              31   \nid3858529       7.661527  3.968634      1    1           11              26   \nid3504673       6.063785  0.923306      4    2           19               0   \nid2181028       6.077642  0.738763      3    5           13              26   \n\n           dropoff_borough  \nid                          \nid2875421               30  \nid2377394                0  \nid3858529                0  \nid3504673                0  \nid2181028               30  \n"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "features = ['vendor_id', 'month', 'day', 'pickup_hour', 'store_and_fwd_flag', 'distance', 'pickup_borough', 'dropoff_borough']\ndef scale_df(df, test=False):\n    if test:\n        customized_df = df[features]\n        x = customized_df.values #returns a numpy array\n    else:\n        customized_df = df[features + ['trip_duration']]\n        x = customized_df.values[:,:-1] #returns a numpy array\n        \n\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    if not test:\n        trip_duration_col = np.array([customized_df['trip_duration'].values])\n        x_scaled = np.concatenate((x_scaled, trip_duration_col.T), 1)\n    scaled_df = pd.DataFrame(x_scaled, columns=customized_df.columns)\n    return scaled_df\nscaled_df = scale_df(new_df)\nprint scaled_df.head()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "   vendor_id  month       day  pickup_hour  store_and_fwd_flag  distance  \\\n0        1.0    0.4  0.000000     0.739130                 0.0  0.001208   \n1        0.0    1.0  1.000000     0.000000                 0.0  0.001455   \n2        1.0    0.0  0.166667     0.478261                 0.0  0.005146   \n3        1.0    0.6  0.333333     0.826087                 0.0  0.001197   \n4        1.0    0.4  0.833333     0.565217                 0.0  0.000958   \n\n   pickup_borough  dropoff_borough  trip_duration  \n0        0.604651         0.652174       6.122493  \n1        0.720930         0.000000       6.498282  \n2        0.604651         0.000000       7.661527  \n3        0.000000         0.000000       6.063785  \n4        0.604651         0.652174       6.077642  \n"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "from sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndef rlmse_func(predicted, actual):\n    return np.sqrt(np.mean(np.square(np.log(predicted+1.0) - np.log(actual+1.0))))\n\nrlmse = make_scorer(rlmse_func, greater_is_better=False)\n\n# param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n#               'max_depth': [4, 6],\n#               'min_samples_leaf': [3, 5, 9, 17],\n#               }\n\nX_train, y_train = scaled_df[features].values, scaled_df[['trip_duration']].values\nxgb_model = XGBRegressor(objective='reg:linear', max_depth=5, learning_rate=0.3, n_estimators=2000, nthread=-1)\nprint -1.0*cross_val_score(xgb_model, X_train, y_train.ravel(), scoring=rlmse, cv=10, verbose=8).mean()\n\n# gs_cv = GridSearchCV(model, param_grid, n_jobs=4, verbose=4, scoring=rmse).fit(X_train, y_train)\n# print gs_cv.best_params_\n# print gs_cv.best_score_", "outputs": [{"output_type": "stream", "name": "stderr", "text": "/home/nbuser/anaconda2_410/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/home/nbuser/anaconda2_410/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV]  ................................................................\n[CV] ................................ , score=-0.065392, total= 7.9min\n[CV]  ................................................................\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.9min remaining:    0.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] ................................ , score=-0.066165, total= 6.9min\n[CV]  ................................................................\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 14.7min remaining:    0.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] ................................ , score=-0.065597, total= 6.8min\n[CV]  ................................................................\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 21.5min remaining:    0.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] ................................ , score=-0.065532, total= 6.9min\n[CV]  ................................................................\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 28.5min remaining:    0.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] ................................ , score=-0.066770, total= 6.9min\n[CV]  ................................................................\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 35.4min remaining:    0.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] ................................ , score=-0.065353, total= 7.7min\n[CV]  ................................................................\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 43.0min remaining:    0.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] ................................ , score=-0.065288, total= 7.0min\n[CV]  ................................................................\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 50.0min remaining:    0.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] ................................ , score=-0.065818, total= 7.9min\n[CV]  ................................................................\n[CV] ................................ , score=-0.065837, total= 7.0min\n[CV]  ................................................................\n[CV] ................................ , score=-0.065744, total= 6.9min\n0.0657493933673\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 71.9min finished\n"}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "test_df = parallelize_dataframe(test_dataframe, transform_df, cpu_count(), cpu_count())\nnew_test_df = scale_df(add_borough(test_df), test=True)\nprint new_test_df.head()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "   vendor_id  month  day  pickup_hour  store_and_fwd_flag  distance  \\\n0        0.0    1.0  0.5          1.0                 0.0  0.002651   \n1        0.0    1.0  0.5          1.0                 0.0  0.002663   \n2        0.0    1.0  0.5          1.0                 0.0  0.001261   \n3        1.0    1.0  0.5          1.0                 0.0  0.005086   \n4        0.0    1.0  0.5          1.0                 0.0  0.000927   \n\n   pickup_borough  dropoff_borough  \n0        0.000000         0.333333  \n1        0.310345         0.366667  \n2        0.000000         0.000000  \n3        0.586207         0.000000  \n4        0.586207         0.700000  \n"}], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "xgb_model.fit(X_train, y_train)\npredictions = xgb_model.predict(new_test_df.values)\nprint predictions", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[ 8.15130615  7.105124    6.1855197  ...,  6.50277948  9.21678448\n  7.09972954]\n"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "test_trip_duration = np.expm1(predictions)\nresult_df = pd.DataFrame({'id': test_dataframe.index.values, 'trip_duration': test_trip_duration})\nresult_df.to_csv('answer.csv', index=False)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}